# bert-distillation

模型压缩技术可以分为两打类：
- 第一类，使用某种手段把大模型改为小模型：剪枝（Pruning）、量化（Quantization）
    - 剪枝，删减模型组件，使其变为较小的模型，但效果不至于变得太差。
    - 量化，不改变模型结构，权重改用其他的数值类型，但效果不至于变得太差。
- 第二类，蒸馏（Distillation），用一个小模型来学习大模型的输出。

这里提供蒸馏的实现。蒸馏的原理是，大模型的输出往往包括非常丰富的信息（logits），而不是简单的one-hot，因此小模型能够从大模型的输出中学习更丰富的信息。


这里提供一个使用CNN蒸馏大模型的例子。
